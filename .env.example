# App
APP_NAME=fastapi-llama-api
DEBUG=true

# Uvicorn (o Dockerfile lê estas variáveis)
HOST=0.0.0.0
PORT=8000
RELOAD=false

# Auth / JWT (PyJWT)
# Gere uma chave forte!
# Ex.: python -c "import secrets; print(secrets.token_urlsafe(48))"
SECRET_KEY=troque-esta-chave-super-secreta
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30

# OpenSearch
# No docker-compose, o host do serviço é "opensearch"
OPENSEARCH_HOST=http://opensearch:9200
OPENSEARCH_USER=admin
OPENSEARCH_PASS=SuaSenhaForte123!
OPENSEARCH_TIMEOUT=15
OPENSEARCH_SHARDS=1
OPENSEARCH_REPLICAS=0
INDEX_NAME=knowledge_base
ADMIN_PASS=SuaSenhaForte123!


# Embeddings / KNN
# Modelo sentence-transformers (compatível com 384 dims)
EMBED_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
# Engine/space do KNN no OpenSearch
KNN_ENGINE=faiss
KNN_SPACE=l2

# LLaMA (binário e modelo)
# Se usar o Dockerfile que compila o llama.cpp, este é o caminho do binário:
LLAMA_CPP_PATH=./llama.cpp/build/bin
# Monte seus modelos em ./models e aponte aqui:
MODEL_PATH=./mistral/mistral-7b-instruct-v0.1.Q4_K_M.gguf
