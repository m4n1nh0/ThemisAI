# FastAPI LLaMA API

Este projeto Ã© uma API baseada no FastAPI que integra o modelo de linguagem LLaMA para geraÃ§Ã£o de respostas e treinamento, armazenando e consultando dados no OpenSearch.

## DefiniÃ§Ã£o do projeto

O projeto que vocÃª estÃ¡ desenvolvendo utiliza uma arquitetura modular, baseada na divisÃ£o de responsabilidades entre diferentes componentes. Vou te explicar cada parte e como ela se integra no processo de utilizaÃ§Ã£o da LLaMA para gerar respostas com base nos resultados da busca no OpenSearch. A estrutura geral do projeto segue o padrÃ£o de desenvolvimento de APIs, com foco na escalabilidade e organizaÃ§Ã£o.

Arquitetura e Fluxo
A arquitetura do projeto segue um padrÃ£o de microserviÃ§os para um backend em FastAPI. A principal responsabilidade Ã© receber as consultas dos usuÃ¡rios, buscar os dados no OpenSearch, e utilizar a LLaMA (um modelo de linguagem) para gerar respostas, com base nesses dados.

### FastAPI como Framework Principal:

O FastAPI Ã© utilizado como o framework principal para criar a API. Ele gerencia as requisiÃ§Ãµes HTTP, validaÃ§Ãµes e responde ao usuÃ¡rio com as informaÃ§Ãµes geradas pelo modelo LLaMA.

### DivisÃ£o em MÃ³dulos:

O projeto segue a arquitetura de mÃ³dulos, onde cada mÃ³dulo tem uma responsabilidade especÃ­fica. A estrutura estÃ¡ dividida em vÃ¡rias pastas (como models, services, routes, etc.) para separar as responsabilidades e facilitar a manutenÃ§Ã£o.

### Busca no OpenSearch:

O serviÃ§o de opensearch_service.py Ã© responsÃ¡vel por interagir com o OpenSearch. Quando o usuÃ¡rio faz uma consulta, esse serviÃ§o se comunica com o OpenSearch para buscar os dados relevantes.

### GeraÃ§Ã£o de Respostas com LLaMA:

A LLaMA Ã© usada dentro do llama_service.py para gerar respostas contextuais. O modelo recebe como entrada o conteÃºdo dos resultados da busca no OpenSearch e gera uma resposta personalizada, de forma eficiente.

### AutenticaÃ§Ã£o e AutorizaÃ§Ã£o:

A parte de auth.py cuida da autenticaÃ§Ã£o dos usuÃ¡rios. Esse mÃ³dulo Ã© responsÃ¡vel por garantir que as requisiÃ§Ãµes sejam feitas de maneira segura e autorizada.

## Fluxo Completo
O usuÃ¡rio faz uma requisiÃ§Ã£o para a API (por exemplo, atravÃ©s de um endpoint como /ask).

A requisiÃ§Ã£o chega ao ask.py, onde Ã© tratada.

O ask.py chama o opensearch_service.py para realizar a busca no OpenSearch.

Os resultados da busca sÃ£o passados para o llama_service.py, que utiliza a LLaMA para gerar uma resposta mais personalizada e contextual.

A resposta gerada Ã© retornada ao usuÃ¡rio.

## ğŸ“‚ Estrutura do Projeto

```
fastapi-llama-api/
â”‚â”€â”€ app/
â”‚   â”œâ”€â”€ config/                  # ConfiguraÃ§Ãµes gerais
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ db/                      # Gerenciamento do banco de dados
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models/                   # Modelos de dados
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ routes/                   # Rotas da API
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ auth.py               # AutenticaÃ§Ã£o
â”‚   â”‚   â”œâ”€â”€ training.py           # Treinamento do modelo
â”‚   â”‚   â”œâ”€â”€ ask.py                # GeraÃ§Ã£o de respostas
â”‚   â”œâ”€â”€ services/                 # ServiÃ§os internos
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ opensearch_service.py # ConexÃ£o com OpenSearch
â”‚   â”‚   â”œâ”€â”€ llama_service.py      # InteraÃ§Ã£o com LLaMA
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                   # Ponto de entrada da API
â”‚â”€â”€ requirements.txt              # DependÃªncias do projeto
â”‚â”€â”€ Dockerfile                    # ConfiguraÃ§Ã£o do Docker
â”‚â”€â”€ docker-compose.yml            # OrquestraÃ§Ã£o de containers
```

## ğŸš€ ConfiguraÃ§Ã£o e ExecuÃ§Ã£o

### PrÃ©-requisitos
- Python 3.10+
- Docker e Docker Compose (caso queira rodar via containers)
- OpenSearch em execuÃ§Ã£o
- Modelo LLaMA disponÃ­vel

### 1ï¸âƒ£ ConfiguraÃ§Ã£o do Ambiente Virtual
```sh
python -m venv venv
source venv/bin/activate  # Linux/macOS
venv\Scripts\activate  # Windows
```

### 2ï¸âƒ£ InstalaÃ§Ã£o das DependÃªncias
```sh
pip install -r requirements.txt
```

### 3ï¸âƒ£ ExecuÃ§Ã£o da API
```sh
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

### 4ï¸âƒ£ ExecuÃ§Ã£o com Docker
```sh
docker-compose up --build
```

## ğŸ› ï¸ Endpoints Principais

### ğŸ”‘ AutenticaÃ§Ã£o (`/auth`)
- `POST /auth/login` â†’ Gera um token de autenticaÃ§Ã£o.

### ğŸ“š Treinamento (`/training`)
- `POST /training/train` â†’ Treina o modelo com novos dados.

### ğŸ¤– GeraÃ§Ã£o de Respostas (`/ask`)
- `POST /ask/question` â†’ Retorna uma resposta baseada no modelo LLaMA.

## ğŸ—ï¸ Tecnologias Utilizadas
- **FastAPI** â†’ Backend
- **LLaMA** â†’ Modelo de linguagem
- **OpenSearch** â†’ Armazenamento e indexaÃ§Ã£o
- **Docker** â†’ ContainerizaÃ§Ã£o

## ğŸ“Œ ContribuiÃ§Ã£o
Se deseja contribuir, faÃ§a um fork do projeto e crie um pull request com suas melhorias!

## ğŸ“ LicenÃ§a
Este projeto segue a licenÃ§a MIT.

